{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import functional as FT\n",
    "from torchvision import transforms as T\n",
    "from torch import nn, optim\n",
    "from torch.nn import  functional as F\n",
    "from torch.utils.data import DataLoader, sampler, random_split, Dataset\n",
    "import copy\n",
    "import math\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A # data augmentation library\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import  tqdm #  progress bar\n",
    "from torchvision.utils import  draw_bounding_boxes\n",
    "from pycocotools.coco import COCO\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (torch.__version__)\n",
    "print (torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(train=False):\n",
    "    if train:\n",
    "        transorm = A.Compose([\n",
    "            A.LongestMaxSize(max_size=1000), # Resize the input image to 413x1000\n",
    "            A.PadIfNeeded(min_height=413, min_width=1000), # Add extra padding if needed\n",
    "            A.HorizontalFlip(p=0.3), # Horizontally flip the image with a probability of 30%\n",
    "            A.VerticalFlip(p=0.3), # Vertically flip the image with a probability of 30%\n",
    "            A.RandomBrightnessContrast(p=0.1), # Randomly change brightness and contrast of the image with a probability of 10%\n",
    "            A.ColorJitter(p=0.1), # Randomly change the brightness, contrast, and saturation of an image with a probability of 10%\n",
    "             A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize\n",
    "            ToTensorV2() # Convert the image to PyTorch tensor\n",
    "        ], bbox_params=A.BboxParams(format='coco')) # Define the format of the bounding boxes\n",
    "    else:\n",
    "        transorm = A.Compose([\n",
    "            A.Resize(413, 1000),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "\n",
    "    return transorm\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LicensePlateDataset(datasets.VisionDataset): # subclass of datasets.VisionDataset\n",
    "    def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        self.split = split # train, valid, test\n",
    "        self.coco = COCO(os.path.join(root, split, \"annotations.json\")) # annotations stored here\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
    "\n",
    "    def _load_image(self, id: int):\n",
    "        path = self.coco.loadImgs(id)[0]['file_name']\n",
    "        image = cv2.imread(os.path.join(self.root, self.split, path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "    \n",
    "    def _load_target(self, id):\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "        \n",
    "        boxes = [t['bbox'] + [t['category_id']] for t in target]  # xywh + class label\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, bboxes=boxes)\n",
    "            image = transformed['image']\n",
    "            boxes = transformed['bboxes']\n",
    "            \n",
    "            new_boxes = []\n",
    "            for box in boxes:\n",
    "                xmin = box[0]\n",
    "                ymin = box[1]\n",
    "                xmax = xmin + box[2]  # Ensure width ≥ 1\n",
    "                ymax = ymin + box[3]  # Ensure height ≥ 1\n",
    "                new_boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
    "\n",
    "            targ = {}  # Target dictionary\n",
    "            targ['boxes'] = boxes\n",
    "            targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n",
    "            targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n",
    "            targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])  \n",
    "            targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
    "\n",
    "            return image.div(255), targ  # Normalize the image\n",
    "        else:\n",
    "            return image, target\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path =\"D:/Projects/VehicleDetectModel\" # set the path to the dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load classes\n",
    "coco = COCO(os.path.join(dataset_path, \"train/annotations.json\")) # load annotations\n",
    "categories = coco.cats # load categories\n",
    "n_classes = len(categories.keys())\n",
    "categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [i[1]['name'] for i in categories.items()] # get the class names\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LicensePlateDataset(root=dataset_path, split=\"train\", transforms=get_transforms(True)) # Load the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "sample = train_dataset[2]\n",
    "img_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\n",
    "# Ensure the number of labels matches the number of boxes\n",
    "labels = [classes[i] for i in sample[1]['labels'].tolist()] * len(sample[1]['boxes'])\n",
    "\n",
    "plt.imshow(draw_bounding_boxes(\n",
    "    img_int, sample[1]['boxes'], labels, width=4\n",
    ").permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "FasterRCNN with a backbone of resnet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the faster rcnn model\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features # number of input features for the box predictor\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)\n",
    "print(model.rpn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collating function for the train dataloader, it allows to create batches of data that can be easily pass into the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can take in the data and that it will not crash during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(train_loader))\n",
    "images = list(image for image in images)\n",
    "\n",
    "# Filter out invalid bounding boxes\n",
    "for target in targets:\n",
    "\tvalid_boxes = []\n",
    "\tvalid_labels = []\n",
    "\tfor box, label in zip(target['boxes'], target['labels']):\n",
    "\t\txmin, ymin, xmax, ymax = box\n",
    "\t\tif xmax > xmin and ymax > ymin:  # Check for positive width and height\n",
    "\t\t\tvalid_boxes.append(box)\n",
    "\t\t\tvalid_labels.append(label)\n",
    "\ttarget['boxes'] = torch.stack(valid_boxes) if valid_boxes else torch.empty((0, 4), dtype=torch.float32)\n",
    "\ttarget['labels'] = torch.tensor(valid_labels, dtype=torch.int64) if valid_labels else torch.empty((0,), dtype=torch.int64)\n",
    "\n",
    "output = model(images, targets)  # Just make sure this runs without error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:\n",
    "\n",
    "The following is a function that will train the model for one epoch. Torchvision Object Detections models have a loss function built in, and it will calculate the loss automatically if you pass in the inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, optimizer, loader, device, epoch):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    all_losses = []\n",
    "    all_losses_dict = []\n",
    "    \n",
    "    for images, targets in tqdm(loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets) # get the loss dictionary\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n",
    "        loss_value = losses.item()\n",
    "        \n",
    "        all_losses.append(loss_value)\n",
    "        all_losses_dict.append(loss_dict_append)\n",
    "        \n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping trainig\") # train if loss becomes infinity\n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # epoch wise mean loss\n",
    "    epoch_loss = np.mean(all_losses)\n",
    "    \n",
    "    # Convert list of dicts to DataFrame and compute mean loss per key\n",
    "    if all_losses_dict:\n",
    "        all_losses_frame = pd.DataFrame(all_losses_dict)\n",
    "        epoch_losses_dict = all_losses_frame.mean().to_dict()\n",
    "    else:\n",
    "        epoch_losses_dict = {}\n",
    "\n",
    "    # Print losses\n",
    "    print(\"Epoch {}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n",
    "        epoch, \n",
    "        epoch_loss,\n",
    "        epoch_losses_dict.get('loss_classifier', 0),\n",
    "        epoch_losses_dict.get('loss_box_reg', 0),\n",
    "        epoch_losses_dict.get('loss_rpn_box_reg', 0),\n",
    "        epoch_losses_dict.get('loss_objectness', 0)\n",
    "    ))\n",
    "\n",
    "    return epoch_loss, epoch_losses_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume 10 Epochs enough to train this model for a high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs= 10\n",
    "train_losses = []\n",
    "loss_classifier = []\n",
    "loss_box_reg = []\n",
    "loss_rpn_box = []\n",
    "loss_objectness = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss, epoch_losses_dict = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "\n",
    "    # Store total and individual losses\n",
    "    train_losses.append(epoch_loss)\n",
    "    loss_classifier.append(epoch_losses_dict['loss_classifier'])\n",
    "    loss_box_reg.append(epoch_losses_dict['loss_box_reg'])\n",
    "    loss_rpn_box.append(epoch_losses_dict['loss_rpn_box_reg'])\n",
    "    loss_objectness.append(epoch_losses_dict['loss_objectness'])\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label=\"Total Loss\", marker=\"o\", linestyle=\"-\")\n",
    "plt.plot(loss_classifier, label=\"Classification Loss\", marker=\"s\", linestyle=\"--\")\n",
    "plt.plot(loss_box_reg, label=\"Box Regression Loss\", marker=\"^\", linestyle=\"-.\")\n",
    "plt.plot(loss_rpn_box, label=\"RPN Box Loss\", marker=\"v\", linestyle=\":\")\n",
    "plt.plot(loss_objectness, label=\"Objectness Loss\", marker=\"x\", linestyle=\"-.\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Trends Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Trying on sample Images\n",
    "\n",
    "This is the inference code for the model. First, we set the model to evaluation mode and clear the GPU Cache. We also load a test dataset, so that we can use fresh images that the model hasn't seen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = LicensePlateDataset(root=dataset_path, split=\"valid\", transforms=get_transforms(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = test_dataset[9]\n",
    "img_int = torch.tensor(img*255, dtype=torch.uint8)\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])\n",
    "    pred = prediction[0]\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "plt.imshow(draw_bounding_boxes(img_int,\n",
    "    pred['boxes'][pred['scores'] > 0.6],\n",
    "    [classes[i] for i in pred['labels'][pred['scores'] > 0.6].tolist()], width=4\n",
    ").permute(1, 2, 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_lecense_plate_detection.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load Faster R-CNN with ResNet-50 backbone\n",
    "def get_model(num_classes):\n",
    "    # Load pre-trained Faster R-CNN\n",
    "    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features # we need to change the head\n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 2 \n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "model = get_model(num_classes)\n",
    "model = torch.load(\"model_lecense_plate_detection.pth\")\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Open image\n",
    "    image_tensor = F.to_tensor(image).unsqueeze(0)  # Convert image to tensor and add batch dimension\n",
    "    return image_tensor.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Load the unseen image\n",
    "image_path = \"test/9fd08d7e-30d2-471b-907b-ab030b3f2eff.jpeg\"\n",
    "image_tensor = prepare_image(image_path)\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    prediction = model(image_tensor)\n",
    "\n",
    "\n",
    "COCO_CLASSES = {1: \"LicensePlate\"}\n",
    "\n",
    "def get_class_name(class_id):\n",
    "    return COCO_CLASSES.get(class_id, \"Background\")\n",
    "    \n",
    "# Draw bounding boxes with the correct class names and increase image size\n",
    "def draw_boxes(image, prediction, fig_size=(10, 10)):\n",
    "    boxes = prediction[0]['boxes'].cpu().numpy()  # Get predicted bounding boxes\n",
    "    labels = prediction[0]['labels'].cpu().numpy()  # Get predicted labels\n",
    "    scores = prediction[0]['scores'].cpu().numpy()  # Get predicted scores\n",
    "    \n",
    "    # Set a threshold for showing boxes (e.g., score > 0.5)\n",
    "    threshold = 0.5\n",
    "    \n",
    "    # Set up the figure size to control the image size\n",
    "    plt.figure(figsize=fig_size)  # Adjust the figure size here\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > threshold:\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            class_name = get_class_name(label)  # Get the class name\n",
    "            plt.imshow(image)  # Display the image\n",
    "            plt.gca().add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "                                              linewidth=2, edgecolor='r', facecolor='none'))\n",
    "            plt.text(x_min, y_min, f\"{class_name} ({score:.2f})\", color='r')\n",
    "    \n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.show()\n",
    "\n",
    "# Display the image with bounding boxes and correct labels\n",
    "draw_boxes(Image.open(image_path), prediction, fig_size=(12, 10))  # Example of increased size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "# Define metric\n",
    "metric = MeanAveragePrecision(iou_thresholds=[0.5, 0.75])  # COCO-style IoU \n",
    "\n",
    "# Evaluate Model on Test Dataset\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_dataset:\n",
    "        images = [img.unsqueeze(0) if len(img.shape) == 2 else img for img in images]  # Ensure each image is 3D\n",
    "        outputs = model(images)  # Run inference\n",
    "\n",
    "        # Convert predictions to required format (list of dictionaries)\n",
    "        predictions = [{\n",
    "            \"boxes\": outputs[0][\"boxes\"].detach().cpu(),\n",
    "            \"scores\": outputs[0][\"scores\"].detach().cpu(),\n",
    "            \"labels\": outputs[0][\"labels\"].detach().cpu()\n",
    "        }]\n",
    "\n",
    "        # Convert ground truths to required format\n",
    "        ground_truths = [{\n",
    "            \"boxes\": targets[\"boxes\"],\n",
    "            \"labels\": targets[\"labels\"]\n",
    "        }]\n",
    "\n",
    "        # Update metric\n",
    "        metric.update(predictions, ground_truths)\n",
    "\n",
    "# Compute final mAP\n",
    "result = metric.compute()\n",
    "print(\"mAP@0.5:\", result[\"map_50\"])\n",
    "print(\"mAP@0.5:0.95:\", result[\"map\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
